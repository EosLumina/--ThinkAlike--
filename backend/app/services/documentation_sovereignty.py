import re
from datetime import datetime
import hashlib
import json
import os
import git
import yaml  # Add explicit import here
from pathlib import Path
from typing import Dict, List, Any, Optional


class DocumentationSovereigntyService:
    """Service that embodies our radical transparency principle for documentation.

    This service ensures documentation sovereignty by:
    1. Creating verifiable integrity proofs for all documentation
    2. Tracking all changes and establishing clear provenance chains
    3. Detecting unauthorized modifications through cryptographic verification
    4. Preserving the knowledge commons against invisible corruption

    The integrity map generated by this service serves as both technical protection
    and philosophical manifestation of our commitment to sovereignty over collective knowledge.
    """

    def __init__(self, db=None, docs_dir=None, integrity_file=None, traceability_service=None):
        """Initialize the Documentation Sovereignty Service.

        Args:
            db: Database connection for persistent storage (optional)
            docs_dir: Path to the documentation directory (defaults to /docs)
            integrity_file: Path to the integrity map file (defaults to /docs/INTEGRITY.json)
            traceability_service: Optional service for recording data access/creation events
        """
        self.db = db
        self.docs_dir = docs_dir or Path('/workspaces/--ThinkAlike--/docs')
        self.integrity_file = integrity_file or Path(
            '/workspaces/--ThinkAlike--/docs/INTEGRITY.json')
        self.traceability = traceability_service

    def generate_integrity_map(self) -> Dict[str, Any]:
        """Create verifiable integrity proofs for all documentation.

        This function walks through all markdown files in the documentation directory,
        generating cryptographic hashes and metadata to establish a verifiable record
        of the documentation's current state.

        Returns:
            Dict containing the integrity map with document hashes and metadata
        """
        integrity_map = {
            "generation_time": datetime.utcnow().isoformat(),
            "docs_root": str(self.docs_dir),
            "documents": {},
            "stats": {
                "total_documents": 0,
                "total_size_bytes": 0,
                "categories": {}
            }
        }

        # Process all markdown files
        for doc_file in self.docs_dir.glob('**/*.md'):
            # Skip files in hidden directories or files starting with .
            if any(part.startswith('.') for part in doc_file.parts) or doc_file.name.startswith('.'):
                continue

            relative_path = str(doc_file.relative_to(self.docs_dir))
            category = relative_path.split(
                '/')[0] if '/' in relative_path else 'uncategorized'

            # Generate hash for integrity verification
            with open(doc_file, 'rb') as f:
                content = f.read()
                doc_hash = hashlib.sha256(content).hexdigest()

            # Get file statistics
            file_size = os.path.getsize(doc_file)
            last_modified = datetime.fromtimestamp(
                os.path.getmtime(doc_file)).isoformat()

            # Get git history if available
            git_history = self._get_git_history(doc_file)

            # Extract metadata from content if available
            metadata = self._extract_metadata(
                content.decode('utf-8', errors='ignore'))

            # Store document metadata
            integrity_map["documents"][relative_path] = {
                "hash": doc_hash,
                "last_modified": last_modified,
                "size_bytes": file_size,
                "git_history": git_history,
                "metadata": metadata
            }

            # Update statistics
            integrity_map["stats"]["total_documents"] += 1
            integrity_map["stats"]["total_size_bytes"] += file_size
            integrity_map["stats"]["categories"][category] = integrity_map["stats"]["categories"].get(
                category, 0) + 1

        # Write integrity map to file
        os.makedirs(self.integrity_file.parent, exist_ok=True)
        with open(self.integrity_file, 'w') as f:
            json.dump(integrity_map, f, indent=2)

        # Record data creation if traceability service is available
        if self.traceability:
            self.traceability.record_data_creation(
                user_id="system",  # Usually would come from authenticated user
                data_type="documentation_integrity_map",
                data_id=str(self.integrity_file),
                purpose="documentation_sovereignty"
            )

        return integrity_map

    def verify_integrity(self) -> Dict[str, Any]:
        """Verify all documentation against the integrity map.

        This function checks if all documents in the integrity map still exist
        and have not been modified. It also identifies any new documents that
        are not yet included in the integrity map.

        Returns:
            Dict containing verification results, including any violations found
        """
        if not self.integrity_file.exists():
            return {
                "status": "no_integrity_map",
                "message": "No integrity map found. Generate one first."
            }

        with open(self.integrity_file, 'r') as f:
            integrity_map = json.load(f)

        results = {
            "verified_at": datetime.utcnow().isoformat(),
            "status": "verified",
            "violations": [],
            "stats": {
                "total_verified": 0,
                "total_violations": 0,
                "new_files": 0
            }
        }

        # Check each document in the integrity map
        for rel_path, expected in integrity_map["documents"].items():
            doc_file = self.docs_dir / rel_path

            # Check if file exists
            if not doc_file.exists():
                results["violations"].append({
                    "file": rel_path,
                    "type": "missing",
                    "details": "Document no longer exists",
                    "expected_hash": expected["hash"]
                })
                results["stats"]["total_violations"] += 1
                continue

            # Verify hash
            with open(doc_file, 'rb') as f:
                content = f.read()
                current_hash = hashlib.sha256(content).hexdigest()

            if current_hash != expected["hash"]:
                results["violations"].append({
                    "file": rel_path,
                    "type": "modified",
                    "details": "Content hash doesn't match integrity map",
                    "expected_hash": expected["hash"],
                    "current_hash": current_hash
                })
                results["stats"]["total_violations"] += 1
            else:
                results["stats"]["total_verified"] += 1

        # Look for new files not in integrity map
        new_files = []
        for doc_file in self.docs_dir.glob('**/*.md'):
            # Skip files in hidden directories or files starting with .
            if any(part.startswith('.') for part in doc_file.parts) or doc_file.name.startswith('.'):
                continue

            rel_path = str(doc_file.relative_to(self.docs_dir))
            if rel_path not in integrity_map["documents"]:
                new_files.append(rel_path)
                results["stats"]["new_files"] += 1

        if new_files:
            results["new_files"] = new_files

        # Update status if violations found
        if results["violations"]:
            results["status"] = "integrity_violated"

        # Record verification if traceability service is available
        if self.traceability:
            self.traceability.record_data_access(
                user_id="system",  # Usually would come from authenticated user
                data_type="documentation_integrity_verification",
                purpose="documentation_sovereignty"
            )

        return results

    def get_document_history(self, doc_path: str) -> Dict[str, Any]:
        """Retrieve the full history for a specific document.

        Args:
            doc_path: Relative path to the document from docs directory

        Returns:
            Dict containing the document's full history and integrity information
        """
        doc_file = self.docs_dir / doc_path
        if not doc_file.exists():
            return {"error": "Document not found", "path": doc_path}

        # Get full git history
        git_history = self._get_git_history(doc_file, max_commits=50)

        # Get current hash
        with open(doc_file, 'rb') as f:
            content = f.read()
            current_hash = hashlib.sha256(content).hexdigest()

        result = {
            "path": doc_path,
            "current_hash": current_hash,
            "last_modified": datetime.fromtimestamp(os.path.getmtime(doc_file)).isoformat(),
            "size_bytes": os.path.getsize(doc_file),
            "git_history": git_history,
            "metadata": self._extract_metadata(content.decode('utf-8', errors='ignore'))
        }

        # Check if this file is in the integrity map
        if self.integrity_file.exists():
            with open(self.integrity_file, 'r') as f:
                integrity_map = json.load(f)

            if doc_path in integrity_map["documents"]:
                result["integrity_status"] = "verified" if integrity_map["documents"][doc_path]["hash"] == current_hash else "modified"
                result["recorded_hash"] = integrity_map["documents"][doc_path]["hash"]
            else:
                result["integrity_status"] = "untracked"
        else:
            result["integrity_status"] = "no_integrity_map"

        return result

    def _get_git_history(self, file_path: Path, max_commits: int = 1) -> Dict[str, Any]:
        """Extract git history for a file if available.

        Args:
            file_path: Path to the file to get history for
            max_commits: Maximum number of commits to retrieve

        Returns:
            Dict containing git history information or empty dict if git not available
        """
        result = {"last_commit": None, "last_author": None, "commits": []}

        try:
            repo = git.Repo(self.docs_dir, search_parent_directories=True)
            commits = list(repo.iter_commits(
                paths=str(file_path), max_count=max_commits))

            if commits:
                # Add the most recent commit info
                result["last_commit"] = {
                    "id": str(commits[0].hexsha),
                    "date": datetime.fromtimestamp(commits[0].committed_date).isoformat(),
                    "message": commits[0].message.strip()
                }

                result["last_author"] = {
                    "name": commits[0].author.name,
                    "email": commits[0].author.email
                }

                # Add all commits if max_commits > 1
                if max_commits > 1:
                    result["commits"] = [
                        {
                            "id": str(commit.hexsha),
                            "date": datetime.fromtimestamp(commit.committed_date).isoformat(),
                            "message": commit.message.strip(),
                            "author": {
                                "name": commit.author.name,
                                "email": commit.author.email
                            }
                        }
                        for commit in commits
                    ]
        except (git.InvalidGitRepositoryError, git.NoSuchPathError, Exception) as e:
            # Git repository not found or other error, return empty history
            pass

        return result

    def _extract_metadata(self, content: str) -> Dict[str, Any]:
        """Extract metadata from document content.

        Looks for YAML frontmatter or specific metadata markers in the content.

        Args:
            content: String content of the document

        Returns:
            Dict containing extracted metadata
        """
        metadata = {}

        # Extract title from first heading
        title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
        if title_match:
            metadata["title"] = title_match.group(1).strip()

        # Count words and estimate reading time
        word_count = len(content.split())
        metadata["word_count"] = word_count
        metadata["reading_time_minutes"] = round(
            word_count / 200)  # Assuming 200 words per minute

        # Extract YAML frontmatter if present
        frontmatter_match = re.search(
            r'^---\s+(.*?)\s+---', content, re.DOTALL)
        if frontmatter_match:
            try:
                import yaml
                yaml_content = frontmatter_match.group(1)
                yaml_metadata = yaml.safe_load(yaml_content)
                if isinstance(yaml_metadata, dict):
                    metadata.update(yaml_metadata)
            except (ImportError, yaml.YAMLError):
                # If yaml parsing fails, continue without frontmatter metadata
                pass

        return metadata
